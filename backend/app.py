# MALWARE SNIPPER - BACKEND API SERVER
# Integrates with VirusTotal API (70+ engines) and NVD for CVE/CVSS data

from flask import Flask, request, jsonify, Response
from flask_cors import CORS
from flask_socketio import SocketIO, emit
from urllib.parse import urlparse
import requests
import os
from dotenv import load_dotenv
from datetime import datetime
import time
import hashlib
import re
import sys
import sqlite3
import feedparser
from apscheduler.schedulers.background import BackgroundScheduler
import atexit

# Add ml directory to path
sys.path.append(os.path.join(os.path.dirname(__file__), 'ml'))

try:
    from ml_service import SimpleMalwareDetector, URLFeatureExtractor
    ML_AVAILABLE = True
    print("âœ… ML Service loaded successfully")
except ImportError as e:
    print(f"âš ï¸ ML Service not available: {e}")
    ML_AVAILABLE = False
    SimpleMalwareDetector = None
    URLFeatureExtractor = None

# Import persistent storage
try:
    from scan_storage import ScanStorage
    scan_storage = ScanStorage()
    print("âœ… Persistent scan storage initialized")
except ImportError as e:
    print(f"âš ï¸ Scan storage not available: {e}")
    scan_storage = None

# Import Unified Risk Engine (multi-layer analysis)
try:
    from risk_engine import UnifiedRiskEngine
    # Get API keys for threat intelligence
    api_keys = {
        'virustotal': os.getenv('VIRUSTOTAL_API_KEY'),
        'abuseipdb': os.getenv('ABUSEIPDB_API_KEY'),
        'alienvault_otx': os.getenv('ALIENVAULT_OTX_KEY'),
        'urlscan': os.getenv('URLSCAN_API_KEY')
    }
    RISK_ENGINE_AVAILABLE = True
    print("âœ… Unified Risk Engine initialized (6-layer analysis)")
except ImportError as e:
    print(f"âš ï¸ Unified Risk Engine not available: {e}")
    RISK_ENGINE_AVAILABLE = False
    UnifiedRiskEngine = None
    api_keys = {}

# Import URL ML Model for advanced malware detection
try:
    sys.path.append(os.path.join(os.path.dirname(__file__), 'ml_advanced'))
    from url_model_predict import predict_url
    URL_ML_MODEL_AVAILABLE = True
    print("âœ… URL ML Model loaded (trained phishing detection)")
except ImportError as e:
    print(f"âš ï¸ URL ML Model not available: {e}")
    print("   Train model with: python ml_training/train_url_model.py")
    URL_ML_MODEL_AVAILABLE = False
    predict_url = None

# Import JavaScript ML Model for JS malware detection
try:
    from js_model_predict import predict_js
    JS_ML_MODEL_AVAILABLE = True
    print("âœ… JavaScript ML Model loaded (trained malware detection)")
except ImportError as e:
    print(f"âš ï¸ JavaScript ML Model not available: {e}")
    print("   Train model with: python ml_js_model/training/train_js_model.py")
    JS_ML_MODEL_AVAILABLE = False
    predict_js = None

load_dotenv()

app = Flask(__name__)
CORS(app, resources={r"/*": {"origins": "*"}})
socketio = SocketIO(app, cors_allowed_origins="*", async_mode='threading')

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# URL SCANNER BLUEPRINT - Real-time Scanning Pipeline
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
try:
    from scanner_routes import url_scanner_bp, register_socketio_handlers
    SCANNER_ROUTES_AVAILABLE = True
    print("âœ… URL Scanner routes loaded (end-to-end pipeline)")
except ImportError as e:
    print(f"âš ï¸ URL Scanner routes not available: {e}")
    SCANNER_ROUTES_AVAILABLE = False
    url_scanner_bp = None

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# REGISTER BLUEPRINTS & HANDLERS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Register URL Scanner Blueprint
if SCANNER_ROUTES_AVAILABLE and url_scanner_bp:
    app.register_blueprint(url_scanner_bp)
    print("âœ… URL Scanner blueprint registered")
    
    # Register SocketIO handlers
    if 'register_socketio_handlers' in dir():
        register_socketio_handlers(socketio)
        print("âœ… SocketIO handlers registered for real-time updates")

# Initialize ML detector if available
ml_detector = SimpleMalwareDetector() if ML_AVAILABLE else None

# Initialize Unified Risk Engine if available
risk_engine = None
if RISK_ENGINE_AVAILABLE and UnifiedRiskEngine:
    risk_engine = UnifiedRiskEngine(api_keys=api_keys, base_ml_detector=ml_detector)
    print("âœ… Multi-layer security analysis engine ready")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CYBERSECURITY NEWS - DYNAMIC FROM NEWSAPI ONLY
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# NOTE: All news fetching is NOW DYNAMIC from NewsAPI
# No database storage - pure live API calls
# See /api/news/newsapi endpoints for live fetching

print("âœ… News system: Dynamic NewsAPI fetching only (no database storage)")

# Skip initial blogs fetch on startup to avoid database lock
# The scheduler will fetch blogs after the server is ready
# try:
#     print("ğŸ“° Fetching initial blogs on startup...")
#     fetch_cybersecurity_news()
# except Exception as e:
#     print(f"âš ï¸ Initial blog fetch warning: {e}")

# Setup APScheduler for periodic blog updates (10-minute interval)
# Disabled for now - use manual fetch via fetch_blogs_now.py
# try:
#     scheduler = BackgroundScheduler()
#     scheduler.add_job(
#         func=fetch_cybersecurity_news,
#         trigger="interval",
#         minutes=10,
#         id='blog_update_job',
#         name='Fetch cybersecurity news every 10 minutes',
#         replace_existing=True,
#         max_instances=1
#     )
#     scheduler.start()
#     print("âœ… Blog scheduler initialized (10-minute interval)")
# except Exception as e:
#     print(f"âš ï¸ Blog scheduler warning (non-critical): {e}")
#     scheduler = None

# Shutdown scheduler gracefully on app exit
def shutdown_scheduler():
    try:
        pass  # scheduler disabled
    except:
        pass

# atexit.register(shutdown_scheduler)  # Disabled

# VirusTotal API configuration
VIRUSTOTAL_API_KEY = os.getenv('VIRUSTOTAL_API_KEY', 'your_api_key_here')
VIRUSTOTAL_URL = 'https://www.virustotal.com/api/v3/urls'

# NVD (National Vulnerability Database) API configuration
NVD_API_URL = 'https://services.nvd.nist.gov/rest/json/cves/2.0'

# Cache for rate limiting and performance
scan_cache = {}
cve_cache = {}
CACHE_DURATION = 3600  # 1 hour
CVE_CACHE_DURATION = 7200  # 2 hours

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# HEALTH CHECK
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.route('/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    return jsonify({
        'status': 'healthy',
        'service': 'Malware Snipper Scanner',
        'timestamp': datetime.now().isoformat()
    })

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BLOG API ENDPOINTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# UNIFIED CYBERSECURITY NEWS FEED - Real-time aggregation (Dynamic NewsAPI only)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# NOTE: Old /api/blogs endpoints REMOVED - using NewsAPI dynamic fetching only

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CYBERSECURITY NEWS - PURE NEWSAPI DYNAMIC FETCHING (NO DATABASE)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.route('/api/cyber-news', methods=['GET'])
def get_unified_cyber_news():
    """
    GET REAL-TIME CYBERSECURITY NEWS FROM NEWSAPI
    Fresh articles on every request - NO CACHING, NO DATABASE
    
    Query Parameters:
    - category: Filter by category (malware, ransomware, vulnerability, threat, breach, all)
    - limit: Number of articles to fetch (default: 30, max: 50)
    
    Example: /api/cyber-news?category=malware&limit=10
    """
    try:
        # Get query parameters
        category_filter = request.args.get('category', 'all').lower()  # Default: all
        limit = int(request.args.get('limit', 30))
        limit = min(limit, 50)  # Cap at 50
        
        print(f"\nğŸ”„ LIVE NEWS REQUEST - Category: {category_filter}, Limit: {limit}")
        
        # Set no-cache headers
        response_headers = {
            'Cache-Control': 'no-store, no-cache, must-revalidate, max-age=0',
            'Pragma': 'no-cache',
            'Expires': '0'
        }
        
        # Define cybersecurity categories
        cyber_categories = ['malware', 'ransomware', 'vulnerability', 'threat', 'breach', 'phishing', 'ddos', 'exploit']
        
        # Validate category filter
        if category_filter not in ['all'] + cyber_categories:
            category_filter = 'all'
        
        # Try to import and use NewsAPI
        try:
            from news_manager import CyberNewsManager
            news_mgr = CyberNewsManager()
            
            # Fetch from NewsAPI - returns a list of articles
            articles_list = news_mgr.fetch_from_newsapi(limit=limit)
            
            if isinstance(articles_list, list) and len(articles_list) > 0:
                # Transform articles to response format
                articles = []
                for article in articles_list:
                    try:
                        article_category = article.get('category', 'general').lower()
                        
                        # Filter by category if specified
                        if category_filter != 'all' and article_category != category_filter:
                            continue
                        
                        articles.append({
                            'title': article.get('title', 'N/A'),
                            'link': article.get('link', article.get('url', '')),
                            'pubDate': article.get('published', article.get('publishedAt', datetime.now().isoformat())),
                            'contentSnippet': article.get('description', '')[:250],
                            'source': article.get('source', 'NewsAPI'),
                            'image': article.get('image_url', ''),
                            'category': article_category,
                            'priority': article.get('priority', 'medium'),
                            'fetched_at': datetime.now().isoformat()
                        })
                    except Exception as ae:
                        print(f"âš ï¸ Error transforming article: {ae}")
                        continue
                
                # Sort by date (newest first)
                articles.sort(key=lambda x: x['pubDate'], reverse=True)
                
                # If filter applied and no results, show message
                if category_filter != 'all' and len(articles) == 0:
                    print(f"âš ï¸ No articles found for category: {category_filter}")
                    return jsonify({
                        'success': False,
                        'error': f'No articles found for category: {category_filter}',
                        'articles': [],
                        'total': 0,
                        'category': category_filter,
                        'message': f'No {category_filter} news available'
                    }), 404
                
                print(f"âœ… SUCCESS: {len(articles)} FRESH articles fetched from NewsAPI (Category: {category_filter})")
                
                return jsonify({
                    'success': True,
                    'articles': articles,
                    'total': len(articles),
                    'category': category_filter,
                    'last_updated': datetime.now().isoformat(),
                    'cached': False,
                    'source': 'NewsAPI',
                    'available_categories': cyber_categories + ['all'],
                    'message': f'LIVE DATA - Fresh {category_filter} articles from NewsAPI'
                }), 200, response_headers
            else:
                print(f"âš ï¸ No articles fetched from NewsAPI, got: {type(articles_list)}")
                return jsonify({
                    'success': False,
                    'error': 'No articles available from NewsAPI',
                    'articles': [],
                    'total': 0,
                    'message': 'No articles fetched from NewsAPI'
                }), 404
        
        except ImportError as ie:
            print(f"âš ï¸ Import error: {ie}")
            import traceback
            traceback.print_exc()
            return jsonify({
                'success': False,
                'error': str(ie),
                'articles': [],
                'total': 0,
                'message': 'NewsAPI module import error'
            }), 500
        
    except Exception as e:
        print(f"âŒ LIVE NEWS API ERROR: {e}")
        import traceback
        traceback.print_exc()
        
        return jsonify({
            'success': False,
            'error': str(e),
            'articles': [],
            'total': 0,
            'cached': False,
            'message': 'Failed to fetch live news'
        }), 500

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DIRECT NEWSAPI ENDPOINT - Fresh from NewsAPI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.route('/api/newsapi', methods=['GET'])
def get_newsapi_articles():
    """
    GET FRESH NEWS DIRECTLY FROM NEWSAPI
    Fetches cybersecurity news directly from NewsAPI endpoint
    No database caching - always fresh
    """
    try:
        from backend.news_manager import CyberNewsManager
        
        limit = request.args.get('limit', 20, type=int)
        limit = min(limit, 50)  # Cap at 50
        
        print(f"\nğŸ“° NEWSAPI REQUEST - Fetching {limit} fresh articles from NewsAPI...")
        
        manager = CyberNewsManager()
        articles = manager.fetch_from_newsapi(limit=limit)
        
        response_headers = {
            'Cache-Control': 'no-store, no-cache, must-revalidate, max-age=0',
            'Pragma': 'no-cache',
            'Expires': '0'
        }
        
        if articles:
            print(f"âœ… SUCCESS: {len(articles)} articles fetched from NewsAPI")
            return jsonify({
                'success': True,
                'source': 'NewsAPI',
                'articles': articles,
                'total': len(articles),
                'fetched_at': datetime.now().isoformat(),
                'cache': 'DISABLED - Direct API',
                'message': f'Fresh articles from NewsAPI (no database, no caching)'
            }), 200, response_headers
        else:
            print("âš ï¸ No articles returned from NewsAPI")
            return jsonify({
                'success': False,
                'source': 'NewsAPI',
                'articles': [],
                'total': 0,
                'message': 'No articles found from NewsAPI'
            }), 204, response_headers
    
    except Exception as e:
        print(f"âŒ NewsAPI ERROR: {e}")
        return jsonify({
            'success': False,
            'error': str(e),
            'articles': [],
            'total': 0,
            'message': 'Failed to fetch from NewsAPI'
        }), 500

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LIVE NEWS STREAMING - SERVER-SENT EVENTS (SSE)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.route('/api/news/live', methods=['GET'])
def get_live_news():
    """
    Fetch live cybersecurity news immediately (instant fetch)
    Redirects to main /api/cyber-news endpoint for pure NewsAPI fetching
    """
    # Simply call the main cyber-news endpoint
    return get_unified_cyber_news()

@app.route('/api/news/stream', methods=['GET'])
def news_stream():
    """
    Server-Sent Events (SSE) endpoint for live streaming news from NewsAPI
    Pushes new articles every 30 minutes automatically
    """
    def generate():
        print("ğŸ”´ SSE Connection established - streaming live news from NewsAPI")
        try:
            import json
            import time
            
            while True:
                try:
                    # Fetch fresh articles from NewsAPI
                    from news_manager import CyberNewsManager
                    news_mgr = CyberNewsManager()
                    articles_list = news_mgr.fetch_from_newsapi(limit=10)
                    
                    if articles_list:
                        articles = []
                        for article in articles_list[:6]:
                            articles.append({
                                'title': article.get('title', 'N/A'),
                                'link': article.get('link', ''),
                                'pubDate': article.get('published', datetime.now().isoformat()),
                                'description': article.get('description', '')[:200],
                                'source': article.get('source', 'NewsAPI'),
                            })
                        
                        print(f"ğŸ”´ SSE Push: {len(articles)} articles - {datetime.now().isoformat()}")
                        
                        # Send SSE event
                        yield f"data: {json.dumps({'articles': articles, 'timestamp': datetime.now().isoformat(), 'source': 'NewsAPI'})}\n\n"
                except Exception as e:
                    print(f"âš ï¸ SSE fetch error: {e}")
                    yield f"data: {json.dumps({'error': str(e)})}\n\n"
                
                # Wait 30 minutes before next update
                time.sleep(1800)  # 30 minutes
                
        except GeneratorExit:
            print("ğŸ”´ SSE Connection closed")
        except Exception as e:
            print(f"âŒ SSE Stream error: {e}")
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
    
    return Response(generate(), mimetype='text/event-stream', headers={
        'Cache-Control': 'no-cache',
        'X-Accel-Buffering': 'no',
        'Connection': 'keep-alive'
    })

@app.route('/scan-url', methods=['POST'])
def scan_url():
    """
    Scan a URL using VirusTotal API
    
    Request body:
    {
        "url": "https://example.com"
    }
    
    Response:
    {
        "url": "https://example.com",
        "threat_level": "SAFE|SUSPICIOUS|MALICIOUS",
        "overall_risk_score": 15.5,
        "stats": {
            "malicious": 0,
            "suspicious": 2,
            "harmless": 68,
            "undetected": 10
        },
        "threat_names": ["Trojan.Generic", "Phishing"],
        "scan_date": "2025-11-05T12:30:00Z"
    }
    """
    try:
        data = request.get_json()
        url_to_scan = data.get('url')
        
        if not url_to_scan:
            return jsonify({'error': 'URL is required'}), 400
        
        # Check cache
        if url_to_scan in scan_cache:
            cached_result, timestamp = scan_cache[url_to_scan]
            if time.time() - timestamp < CACHE_DURATION:
                print(f"âœ… Returning cached result for {url_to_scan}")
                return jsonify(cached_result)
        
        print(f"ğŸ” Scanning URL: {url_to_scan}")
        
        # Submit URL to VirusTotal
        result = scan_with_virustotal(url_to_scan)
        
        # Cache the result
        scan_cache[url_to_scan] = (result, time.time())
        
        return jsonify(result)
        
    except Exception as e:
        print(f"âŒ Error scanning URL: {str(e)}")
        return jsonify({
            'error': str(e),
            'url': url_to_scan if 'url_to_scan' in locals() else 'unknown',
            'threat_level': 'UNKNOWN',
            'overall_risk_score': 0,
            'stats': {
                'malicious': 0,
                'suspicious': 0,
                'harmless': 0,
                'undetected': 0
            },
            'threat_names': [],
            'scan_date': datetime.now().isoformat()
        }), 500

def scan_with_virustotal(url):
    """Scan URL with VirusTotal API"""
    
    # Check if API key is configured
    if VIRUSTOTAL_API_KEY == 'your_api_key_here' or not VIRUSTOTAL_API_KEY:
        print("âš ï¸ WARNING: VirusTotal API key not configured. Using mock data.")
        return generate_mock_result(url)
    
    print(f"ğŸ” Scanning {url} with VirusTotal API (Key: ...{VIRUSTOTAL_API_KEY[-8:]})")
    
    try:
        headers = {
            'x-apikey': VIRUSTOTAL_API_KEY
        }
        
        # Step 1: Submit URL for scanning
        print(f"ğŸ“¤ Submitting URL to VirusTotal...")
        response = requests.post(
            VIRUSTOTAL_URL,
            headers=headers,
            data={'url': url}
        )
        
        print(f"ğŸ“¡ VirusTotal response status: {response.status_code}")
        
        if response.status_code == 200:
            submission_data = response.json()
            url_id = submission_data['data']['id']
            print(f"âœ… URL submitted, ID: {url_id}")
            
            # Step 2: Get analysis results
            print(f"â³ Waiting for analysis (15 seconds)...")
            time.sleep(15)  # VirusTotal needs time to scan
            
            print(f"ğŸ“¥ Fetching analysis results...")
            analysis_response = requests.get(
                f'{VIRUSTOTAL_URL}/{url_id}',
                headers=headers
            )
            
            print(f"ğŸ“¡ Analysis response status: {analysis_response.status_code}")
            
            if analysis_response.status_code == 200:
                analysis_data = analysis_response.json()
                result = parse_virustotal_result(url, analysis_data)
                print(f"âœ… VirusTotal scan complete: {result['threat_level']}")
                print(f"   Malicious: {result['stats']['malicious']}, Suspicious: {result['stats']['suspicious']}")
                return result
        
        # If API call fails, return mock data with warning
        print(f"âš ï¸ VirusTotal API returned status {response.status_code}, using mock data")
        if response.status_code == 401:
            print(f"âŒ API Key authentication failed! Check your VirusTotal API key.")
        elif response.status_code == 429:
            print(f"âš ï¸ Rate limit exceeded. Using cached/mock data.")
        
        return generate_mock_result(url)
        
    except Exception as e:
        print(f"âŒ VirusTotal API error: {str(e)}")
        print(f"   Falling back to mock data")
        return generate_mock_result(url)

def parse_virustotal_result(url, data):
    """Parse VirusTotal API response"""
    try:
        attributes = data['data']['attributes']
        last_analysis_stats = attributes.get('last_analysis_stats', {})
        last_analysis_results = attributes.get('last_analysis_results', {})
        
        # Calculate stats
        malicious = last_analysis_stats.get('malicious', 0)
        suspicious = last_analysis_stats.get('suspicious', 0)
        harmless = last_analysis_stats.get('harmless', 0)
        undetected = last_analysis_stats.get('undetected', 0)
        
        total_scans = malicious + suspicious + harmless + undetected
        
        # Calculate risk score
        if total_scans > 0:
            risk_score = ((malicious * 2 + suspicious) / total_scans) * 100
        else:
            risk_score = 0
        
        # Determine threat level
        if malicious > 0:
            threat_level = 'MALICIOUS'
        elif suspicious > 3 or risk_score > 15:
            threat_level = 'SUSPICIOUS'
        else:
            threat_level = 'SAFE'
        
        # Extract threat names
        threat_names = []
        for engine, result in last_analysis_results.items():
            if result.get('category') in ['malicious', 'suspicious']:
                threat_name = result.get('result', 'Unknown threat')
                if threat_name and threat_name not in threat_names:
                    threat_names.append(threat_name)
        
        return {
            'url': url,
            'threat_level': threat_level,
            'overall_risk_score': round(risk_score, 2),
            'stats': {
                'malicious': malicious,
                'suspicious': suspicious,
                'harmless': harmless,
                'undetected': undetected
            },
            'threat_names': threat_names[:10],  # Limit to top 10
            'scan_date': datetime.now().isoformat()
        }
        
    except Exception as e:
        print(f"âŒ Error parsing VirusTotal result: {str(e)}")
        return generate_mock_result(url)

def generate_mock_result(url):
    """Generate realistic mock scan result based on URL analysis"""
    import random
    
    url_lower = url.lower()
    domain = urlparse(url).netloc.lower()
    
    print(f"ğŸ­ Generating mock result for: {url}")
    
    # Known safe domains (major legitimate sites)
    safe_domains = [
        'google.com', 'facebook.com', 'youtube.com', 'amazon.com', 'amazon.in',
        'twitter.com', 'instagram.com', 'linkedin.com', 'microsoft.com',
        'apple.com', 'github.com', 'stackoverflow.com', 'wikipedia.org',
        'reddit.com', 'netflix.com', 'zoom.us', 'dropbox.com'
    ]
    
    # Suspicious patterns (potentially unsafe)
    suspicious_patterns = [
        'free-', 'download-', 'click-here', 'verify-account', 'update-',
        'secure-login', 'prize', 'winner', 'congratulations', 'limited-offer',
        'act-now', 'urgent', 'suspended', 'confirm', 'billing'
    ]
    
    # Malicious patterns (clearly dangerous)
    malicious_patterns = [
        'phishing', 'malware', 'virus', 'hack', 'crack', 'keygen',
        'warez', 'torrent-', 'fake-', 'scam', 'phish-', 'exploit'
    ]
    
    # Check if it's a known safe domain
    is_safe_domain = any(safe_domain in domain for safe_domain in safe_domains)
    is_suspicious = any(pattern in url_lower for pattern in suspicious_patterns)
    is_malicious = any(pattern in url_lower for pattern in malicious_patterns)
    
    # Determine threat level
    if is_malicious:
        threat_level = 'MALICIOUS'
        malicious = random.randint(8, 25)
        suspicious = random.randint(5, 12)
        harmless = random.randint(40, 55)
        undetected = random.randint(5, 15)
        risk_score = random.uniform(65, 98)
        threat_names = [
            'Phishing.Generic',
            'Malware.Trojan',
            'Threat.Detected',
            'Malicious.Link',
            'Phishing.URL'
        ]
        print(f"   âš ï¸ MALICIOUS pattern detected!")
        
    elif is_suspicious:
        threat_level = 'SUSPICIOUS'
        malicious = random.randint(0, 3)
        suspicious = random.randint(5, 15)
        harmless = random.randint(55, 68)
        undetected = random.randint(8, 18)
        risk_score = random.uniform(25, 55)
        threat_names = [
            'PUP.Optional.Generic',
            'Adware.Bundler',
            'Potentially.Unwanted'
        ]
        print(f"   âš ï¸ SUSPICIOUS pattern detected!")
        
    elif is_safe_domain:
        threat_level = 'SAFE'
        malicious = 0
        suspicious = random.randint(0, 1)
        harmless = random.randint(70, 82)
        undetected = random.randint(3, 10)
        risk_score = random.uniform(0, 5)
        threat_names = []
        print(f"   âœ… Known safe domain")
        
    else:
        # Unknown domain - mostly safe with small suspicion
        threat_level = 'SAFE'
        malicious = 0
        suspicious = random.randint(0, 3)
        harmless = random.randint(65, 78)
        undetected = random.randint(5, 15)
        risk_score = random.uniform(2, 12)
        threat_names = []
        print(f"   âœ… No threats detected (unknown domain)")
    
    # Generate CVE/CVSS information for threats
    vulnerabilities = []
    if threat_level in ['MALICIOUS', 'SUSPICIOUS']:
        # Generate realistic vulnerability data
        num_vulns = random.randint(1, 3) if threat_level == 'MALICIOUS' else random.randint(0, 1)
        
        sample_cves = [
            {
                'id': 'CVE-2024-1234',
                'severity': 'CRITICAL',
                'cvss_score': random.uniform(9.0, 10.0),
                'description': 'Remote code execution vulnerability in malicious script',
                'remediation': 'Do not visit this website. Block domain in firewall/DNS.'
            },
            {
                'id': 'CVE-2024-5678',
                'severity': 'HIGH',
                'cvss_score': random.uniform(7.0, 8.9),
                'description': 'Cross-site scripting (XSS) vulnerability detected',
                'remediation': 'Avoid entering credentials. Use updated browser with XSS protection.'
            },
            {
                'id': 'CVE-2024-9012',
                'severity': 'MEDIUM',
                'cvss_score': random.uniform(4.0, 6.9),
                'description': 'Suspicious redirect chain detected',
                'remediation': 'Verify URL authenticity. Enable anti-phishing browser extension.'
            }
        ]
        
        for i in range(num_vulns):
            cve = sample_cves[i % len(sample_cves)].copy()
            cve['cvss_score'] = round(cve['cvss_score'], 1)
            vulnerabilities.append(cve)
    
    result = {
        'url': url,
        'threat_level': threat_level,
        'overall_risk_score': round(risk_score, 2),
        'stats': {
            'malicious': malicious,
            'suspicious': suspicious,
            'harmless': harmless,
            'undetected': undetected
        },
        'threat_names': threat_names,
        'vulnerabilities': vulnerabilities,  # NEW: CVE/CVSS information
        'scan_date': datetime.now().isoformat(),
        'mock': True,
        'url_hash': hashlib.md5(url.encode()).hexdigest()[:16],
        'engine_count': malicious + suspicious + harmless + undetected,
        'category': determine_url_category(url)
    }
    
    print(f"   Result: {threat_level} (Risk: {risk_score:.1f}%, CVEs: {len(vulnerabilities)})")
    return result

def determine_url_category(url):
    """Determine URL category for classification"""
    domain = urlparse(url).netloc.lower()
    
    categories = {
        'social': ['facebook', 'twitter', 'instagram', 'linkedin', 'reddit'],
        'shopping': ['amazon', 'ebay', 'alibaba', 'etsy', 'walmart'],
        'search': ['google', 'bing', 'yahoo', 'duckduckgo'],
        'streaming': ['youtube', 'netflix', 'hulu', 'spotify', 'twitch'],
        'technology': ['github', 'stackoverflow', 'microsoft', 'apple'],
        'news': ['cnn', 'bbc', 'reuters', 'nytimes', 'theguardian']
    }
    
    for category, keywords in categories.items():
        if any(keyword in domain for keyword in keywords):
            return category
    
    return 'general'

def fetch_cve_data(threat_names):
    """
    Fetch CVE/CVSS data from NVD API for detected threats
    
    Args:
        threat_names: List of threat names detected by VirusTotal
    
    Returns:
        List of CVE records with CVSS scores and details
    """
    if not threat_names:
        return []
    
    cve_records = []
    
    try:
        # Extract potential CVE IDs from threat names
        cve_pattern = r'CVE-\d{4}-\d{4,7}'
        cve_ids = []
        
        for threat in threat_names:
            matches = re.findall(cve_pattern, str(threat))
            cve_ids.extend(matches)
        
        # If no CVE IDs found, search by keywords
        if not cve_ids:
            # Search for common vulnerability keywords
            keywords = ['malware', 'phishing', 'trojan', 'ransomware', 'exploit']
            for keyword in keywords:
                if any(keyword.lower() in str(t).lower() for t in threat_names):
                    # Fetch recent CVEs for this threat type
                    cve_records.extend(fetch_cves_by_keyword(keyword))
                    break
        else:
            # Fetch specific CVEs
            for cve_id in cve_ids[:5]:  # Limit to 5 CVEs
                cve_data = fetch_single_cve(cve_id)
                if cve_data:
                    cve_records.append(cve_data)
        
        return cve_records[:10]  # Return max 10 CVEs
        
    except Exception as e:
        print(f"âŒ Error fetching CVE data: {str(e)}")
        return []

def fetch_single_cve(cve_id):
    """Fetch a single CVE record from NVD"""
    cache_key = f"cve_{cve_id}"
    
    # Check cache
    if cache_key in cve_cache:
        cached_time, cached_data = cve_cache[cache_key]
        if time.time() - cached_time < CVE_CACHE_DURATION:
            return cached_data
    
    try:
        response = requests.get(
            f"{NVD_API_URL}?cveId={cve_id}",
            timeout=10
        )
        
        if response.status_code == 200:
            data = response.json()
            if data.get('vulnerabilities'):
                vuln = data['vulnerabilities'][0]['cve']
                
                # Extract CVSS scores
                cvss_data = {}
                if 'metrics' in vuln:
                    metrics = vuln['metrics']
                    if 'cvssMetricV31' in metrics:
                        cvss_v3 = metrics['cvssMetricV31'][0]['cvssData']
                        cvss_data = {
                            'version': '3.1',
                            'score': cvss_v3.get('baseScore', 0),
                            'severity': cvss_v3.get('baseSeverity', 'UNKNOWN'),
                            'vector': cvss_v3.get('vectorString', ''),
                            'attackVector': cvss_v3.get('attackVector', ''),
                            'attackComplexity': cvss_v3.get('attackComplexity', ''),
                            'privilegesRequired': cvss_v3.get('privilegesRequired', ''),
                            'userInteraction': cvss_v3.get('userInteraction', ''),
                            'scope': cvss_v3.get('scope', ''),
                            'confidentialityImpact': cvss_v3.get('confidentialityImpact', ''),
                            'integrityImpact': cvss_v3.get('integrityImpact', ''),
                            'availabilityImpact': cvss_v3.get('availabilityImpact', '')
                        }
                    elif 'cvssMetricV2' in metrics:
                        cvss_v2 = metrics['cvssMetricV2'][0]['cvssData']
                        cvss_data = {
                            'version': '2.0',
                            'score': cvss_v2.get('baseScore', 0),
                            'severity': 'MEDIUM',  # V2 doesn't have severity
                            'vector': cvss_v2.get('vectorString', ''),
                            'accessVector': cvss_v2.get('accessVector', ''),
                            'accessComplexity': cvss_v2.get('accessComplexity', ''),
                            'authentication': cvss_v2.get('authentication', '')
                        }
                
                # Extract description
                descriptions = vuln.get('descriptions', [])
                description = next(
                    (d['value'] for d in descriptions if d['lang'] == 'en'),
                    'No description available'
                )
                
                cve_record = {
                    'id': cve_id,
                    'description': description,
                    'cvss': cvss_data,
                    'published': vuln.get('published', ''),
                    'lastModified': vuln.get('lastModified', ''),
                    'references': [
                        {
                            'url': ref.get('url', ''),
                            'source': ref.get('source', ''),
                            'tags': ref.get('tags', [])
                        }
                        for ref in vuln.get('references', [])[:5]
                    ]
                }
                
                # Cache the result
                cve_cache[cache_key] = (time.time(), cve_record)
                return cve_record
        
        return None
        
    except Exception as e:
        print(f"âŒ Error fetching CVE {cve_id}: {str(e)}")
        return None

def fetch_cves_by_keyword(keyword):
    """Fetch recent CVEs by keyword search"""
    try:
        # For demo, return mock CVE data based on threat type
        mock_cves = {
            'malware': {
                'id': 'CVE-2024-1234',
                'description': 'Critical malware vulnerability allowing remote code execution',
                'cvss': {
                    'version': '3.1',
                    'score': 9.8,
                    'severity': 'CRITICAL',
                    'vector': 'CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H',
                    'attackVector': 'NETWORK',
                    'attackComplexity': 'LOW',
                    'privilegesRequired': 'NONE',
                    'userInteraction': 'NONE'
                },
                'published': datetime.now().isoformat(),
                'references': []
            },
            'phishing': {
                'id': 'CVE-2024-5678',
                'description': 'Phishing attack exploiting browser vulnerability',
                'cvss': {
                    'version': '3.1',
                    'score': 7.5,
                    'severity': 'HIGH',
                    'vector': 'CVSS:3.1/AV:N/AC:L/PR:N/UI:R/S:U/C:H/I:H/A:N',
                    'attackVector': 'NETWORK',
                    'attackComplexity': 'LOW',
                    'userInteraction': 'REQUIRED'
                },
                'published': datetime.now().isoformat(),
                'references': []
            }
        }
        
        return [mock_cves.get(keyword.lower(), mock_cves['malware'])]
        
    except Exception as e:
        print(f"âŒ Error searching CVEs by keyword: {str(e)}")
        return []

@app.route('/get-cve-details', methods=['POST'])
def get_cve_details():
    """
    Get CVE/CVSS details for threat analysis
    
    Request body:
    {
        "threat_names": ["Trojan.Generic", "Malware.Detected"],
        "cve_ids": ["CVE-2024-1234"] (optional)
    }
    
    Response:
    {
        "cves": [
            {
                "id": "CVE-2024-1234",
                "description": "...",
                "cvss": {...},
                "references": [...]
            }
        ]
    }
    """
    try:
        data = request.get_json()
        threat_names = data.get('threat_names', [])
        cve_ids = data.get('cve_ids', [])
        
        # Combine threat names and explicit CVE IDs
        all_threats = list(set(threat_names + cve_ids))
        
        cve_data = fetch_cve_data(all_threats)
        
        return jsonify({
            'cves': cve_data,
            'count': len(cve_data),
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({
            'error': str(e),
            'cves': [],
            'count': 0
        }), 500

# ============================================================================
# NEW API ENDPOINTS FOR MALWARESNIPPER
# ============================================================================

# In-memory scan history storage (replace with database in production)
# In-memory database for scan history (replace with actual DB in production)
# NO SAMPLE DATA - ONLY REAL SCANS FROM EXTENSION
scan_history_db = []

# NO SAMPLE DATA - Backend starts EMPTY and waits for REAL scans from extension
print("ğŸ›¡ï¸ Backend initialized - Ready for REAL-TIME scanning")
print("ğŸ“Š Scan database: EMPTY (no dummy data)")
print("â³ Waiting for extension to send URLs for scanning...")

@app.route('/api/scan-url', methods=['POST'])
def api_scan_url():
    """
    Enhanced scan endpoint with history tracking
    
    POST /api/scan-url
    Body: {"url": "https://example.com"}
    
    Returns: Full scan result with threat analysis
    """
    try:
        data = request.get_json()
        url_to_scan = data.get('url')
        
        if not url_to_scan:
            return jsonify({'error': 'URL is required'}), 400
        
        # Sanitize URL
        url_to_scan = url_to_scan.strip()
        
        # Perform scan
        result = scan_with_virustotal(url_to_scan)
        
        # Generate URL hash for unique identification
        url_hash = hashlib.sha256(url_to_scan.encode()).hexdigest()[:16]
        
        # Add to history
        scan_record = {
            'url_hash': url_hash,
            'url': url_to_scan,
            'threat_level': result['threat_level'],
            'risk_score': result['overall_risk_score'],
            'stats': result['stats'],
            'threat_names': result['threat_names'],
            'scan_date': result['scan_date'],
            'timestamp': time.time()
        }
        
        # Keep only last 100 scans
        scan_history_db.insert(0, scan_record)
        if len(scan_history_db) > 100:
            scan_history_db.pop()
        
        return jsonify(result)
        
    except Exception as e:
        return jsonify({
            'error': str(e),
            'url': url_to_scan if 'url_to_scan' in locals() else 'unknown',
            'threat_level': 'UNKNOWN'
        }), 500

@app.route('/api/scan-history', methods=['GET'])
def api_scan_history():
    """
    Get paginated scan history
    
    GET /api/scan-history?page=1&per_page=20
    
    Returns: List of historical scans with pagination
    """
    try:
        page = int(request.args.get('page', 1))
        per_page = min(int(request.args.get('per_page', 20)), 100)  # Max 100 per page
        
        start_idx = (page - 1) * per_page
        end_idx = start_idx + per_page
        
        paginated_results = scan_history_db[start_idx:end_idx]
        
        return jsonify({
            'scans': paginated_results,
            'total': len(scan_history_db),
            'page': page,
            'per_page': per_page,
            'total_pages': (len(scan_history_db) + per_page - 1) // per_page
        })
        
    except Exception as e:
        return jsonify({'error': str(e), 'scans': []}), 500

@app.route('/api/scan-details/<url_hash>', methods=['GET'])
def api_scan_details(url_hash):
    """
    Get detailed scan information by URL hash
    
    GET /api/scan-details/abc123def456
    
    Returns: Full threat intelligence for specific scan
    """
    try:
        # Find scan in history
        scan = next((s for s in scan_history_db if s['url_hash'] == url_hash), None)
        
        if not scan:
            return jsonify({'error': 'Scan not found'}), 404
        
        # Fetch CVE data if threats detected
        cve_data = []
        if scan['threat_names']:
            cve_data = fetch_cve_data(scan['threat_names'])
        
        return jsonify({
            **scan,
            'cve_data': cve_data,
            'remediation': get_remediation_steps(scan['threat_level'], scan['threat_names'])
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/dashboard-stats', methods=['GET'])
def api_dashboard_stats():
    """
    Get real-time dashboard statistics
    
    GET /api/dashboard-stats
    
    Returns: Aggregated metrics for dashboard overview
    """
    try:
        total_scans = len(scan_history_db)
        
        # Calculate statistics
        safe_count = sum(1 for s in scan_history_db if s['threat_level'] == 'SAFE')
        suspicious_count = sum(1 for s in scan_history_db if s['threat_level'] == 'SUSPICIOUS')
        malicious_count = sum(1 for s in scan_history_db if s['threat_level'] == 'MALICIOUS')
        
        # Calculate average risk score
        avg_risk = (
            sum(s['risk_score'] for s in scan_history_db) / total_scans
            if total_scans > 0 else 0
        )
        
        # Get recent scans (last 24 hours simulation)
        recent_scans = scan_history_db[:20]
        
        return jsonify({
            'total_scans': total_scans,
            'safe': safe_count,
            'suspicious': suspicious_count,
            'malicious': malicious_count,
            'average_risk_score': round(avg_risk, 2),
            'detection_rate': round((safe_count / total_scans * 100) if total_scans > 0 else 0, 1),
            'recent_scans': recent_scans,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/threat-statistics', methods=['GET'])
def api_threat_statistics():
    """
    Get threat breakdown by type and severity
    
    GET /api/threat-statistics
    
    Returns: Detailed threat analytics
    """
    try:
        # Categorize threats by type
        threat_types = {}
        severity_breakdown = {
            'CRITICAL': 0,
            'HIGH': 0,
            'MEDIUM': 0,
            'LOW': 0,
            'SAFE': 0
        }
        
        for scan in scan_history_db:
            # Count by threat level
            level = scan['threat_level']
            if level == 'MALICIOUS':
                severity_breakdown['CRITICAL'] += 1
            elif level == 'SUSPICIOUS':
                severity_breakdown['MEDIUM'] += 1
            else:
                severity_breakdown['SAFE'] += 1
            
            # Categorize by threat names
            for threat_name in scan['threat_names']:
                # Extract threat category (first word before dot)
                category = threat_name.split('.')[0] if '.' in threat_name else threat_name
                threat_types[category] = threat_types.get(category, 0) + 1
        
        # Get top domains scanned
        domain_stats = {}
        for scan in scan_history_db:
            try:
                from urllib.parse import urlparse
                domain = urlparse(scan['url']).netloc
                domain_stats[domain] = domain_stats.get(domain, 0) + 1
            except:
                pass
        
        # Sort and get top 10
        top_domains = sorted(domain_stats.items(), key=lambda x: x[1], reverse=True)[:10]
        
        return jsonify({
            'threat_types': threat_types,
            'severity_breakdown': severity_breakdown,
            'top_domains': [{'domain': d[0], 'count': d[1]} for d in top_domains],
            'total_threats_detected': sum(1 for s in scan_history_db if s['threat_level'] != 'SAFE'),
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/weekly-threat-trend', methods=['GET'])
def api_weekly_threat_trend():
    """
    Get weekly threat trend based on real scan history
    
    GET /api/weekly-threat-trend
    
    Returns: Array of daily threat counts for the last 7 days
    """
    try:
        from datetime import datetime, timedelta
        from collections import defaultdict
        
        # Get current date
        now = datetime.now()
        
        # Initialize last 7 days with day names
        day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']
        week_data = []
        
        # Create structure for last 7 days (today and 6 days before)
        for i in range(6, -1, -1):
            date = now - timedelta(days=i)
            day_name = day_names[date.weekday()]
            day_key = date.strftime('%Y-%m-%d')
            week_data.append({
                'date': day_name,
                'threats': 0,
                'day_key': day_key
            })
        
        # Count threats per day from scan history
        for scan in scan_history_db:
            try:
                # Parse scan timestamp
                timestamp_str = scan.get('timestamp', scan.get('scanned_at', ''))
                if not timestamp_str:
                    continue
                
                # Parse timestamp - handle different formats
                try:
                    if 'T' in timestamp_str or '+' in timestamp_str:
                        # ISO format: 2024-11-05T10:30:00 or with timezone
                        scan_time = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
                    else:
                        # Try standard format: 2024-11-05 10:30:00
                        scan_time = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')
                except:
                    continue
                
                # Check if it's a threat (SUSPICIOUS or MALICIOUS)
                threat_level = scan.get('threat_level', 'SAFE')
                if threat_level in ['SUSPICIOUS', 'MALICIOUS']:
                    scan_date = scan_time.strftime('%Y-%m-%d')
                    
                    # Find matching day in week_data and increment
                    for day in week_data:
                        if day['day_key'] == scan_date:
                            day['threats'] += 1
                            break
            except Exception as e:
                print(f"âš ï¸  Error processing scan for weekly trend: {e}")
                continue
        
        # Remove day_key before returning (keep only date and threats)
        result = [{'date': d['date'], 'threats': d['threats']} for d in week_data]
        
        print(f"ğŸ“Š Weekly threat trend generated: {result}")
        return jsonify(result)
        
    except Exception as e:
        print(f"âŒ Error generating weekly threat trend: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/vulnerability-feed', methods=['GET'])
def api_vulnerability_feed():
    """
    Get aggregated cybersecurity news and vulnerability feeds
    
    GET /api/vulnerability-feed
    
    Returns: Latest security news (cached for 10 seconds)
    """
    cache_key = 'vuln_feed'
    
    # Check cache
    if cache_key in scan_cache:
        cached_data, timestamp = scan_cache[cache_key]
        if time.time() - timestamp < 10:  # 10-second cache
            return jsonify(cached_data)
    
    try:
        # Mock vulnerability feed (replace with real API calls in production)
        feed_items = [
            {
                'title': 'Critical Zero-Day Vulnerability Discovered in Popular Browser Extension',
                'source': 'CISA',
                'severity': 'CRITICAL',
                'published': datetime.now().isoformat(),
                'url': 'https://www.cisa.gov/alerts',
                'description': 'A critical vulnerability has been identified that could allow remote code execution.'
            },
            {
                'title': 'New Phishing Campaign Targets Financial Institutions',
                'source': 'TryHackMe',
                'severity': 'HIGH',
                'published': datetime.now().isoformat(),
                'url': 'https://tryhackme.com/news',
                'description': 'Attackers are using sophisticated social engineering tactics.'
            },
            {
                'title': 'Ransomware Group Exploits Unpatched CVE-2024-1234',
                'source': 'HackerOne',
                'severity': 'HIGH',
                'published': datetime.now().isoformat(),
                'url': 'https://hackerone.com/reports',
                'description': 'Organizations urged to patch immediately.'
            },
            {
                'title': 'Malware Distribution via Compromised NPM Packages',
                'source': 'NVD',
                'severity': 'MEDIUM',
                'published': datetime.now().isoformat(),
                'url': 'https://nvd.nist.gov',
                'description': 'Several popular JavaScript packages found to contain malicious code.'
            },
            {
                'title': 'Security Update: Browser Isolation Technology Improvements',
                'source': 'CISA',
                'severity': 'LOW',
                'published': datetime.now().isoformat(),
                'url': 'https://www.cisa.gov',
                'description': 'New standards for browser security announced.'
            }
        ]
        
        result = {
            'items': feed_items,
            'total': len(feed_items),
            'timestamp': datetime.now().isoformat(),
            'sources': ['CISA', 'HackerOne', 'TryHackMe', 'NVD']
        }
        
        # Cache result
        scan_cache[cache_key] = (result, time.time())
        
        return jsonify(result)
        
    except Exception as e:
        return jsonify({'error': str(e), 'items': []}), 500

@app.route('/api/recent-scans', methods=['GET'])
def get_recent_scans():
    """
    Get recent scans for dashboard catch-up sync.
    Accepts optional 'since' timestamp parameter to fetch scans after that time.
    Example: GET /api/recent-scans?since=1730841234567
    """
    try:
        # Get 'since' parameter (timestamp in milliseconds)
        since_param = request.args.get('since', type=int)
        
        if since_param:
            # Convert milliseconds to datetime
            since_datetime = datetime.fromtimestamp(since_param / 1000.0)
            
            # Filter scans that occurred after the 'since' timestamp
            recent_scans = [
                scan for scan in scan_history_db
                if datetime.fromisoformat(scan.get('timestamp', scan.get('scanned_at', ''))) > since_datetime
            ]
        else:
            # If no 'since' parameter, return last 20 scans
            recent_scans = scan_history_db[-20:]
        
        # Sort by timestamp descending (most recent first)
        recent_scans.sort(
            key=lambda x: x.get('timestamp', x.get('scanned_at', '')),
            reverse=True
        )
        
        return jsonify({
            'scans': recent_scans,
            'total': len(recent_scans),
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({'error': str(e), 'scans': []}), 500

def get_remediation_steps(threat_level, threat_names):
    """Generate remediation steps based on threat analysis"""
    steps = []
    
    if threat_level == 'MALICIOUS':
        steps = [
            'IMMEDIATELY close this website and clear your browser cache',
            'Run a full system antivirus scan',
            'Change passwords for any accounts accessed on this device',
            'Check for unauthorized transactions or account changes',
            'Report the malicious URL to authorities',
            'Consider professional security assessment if data was entered'
        ]
    elif threat_level == 'SUSPICIOUS':
        steps = [
            'Do not enter any personal or financial information',
            'Verify the website URL and SSL certificate',
            'Use caution with downloads or form submissions',
            'Enable browser security features',
            'Monitor your accounts for unusual activity',
            'Report suspicious behavior to website administrators'
        ]
    else:
        steps = [
            'Website appears safe, but remain vigilant',
            'Keep your browser and security software updated',
            'Verify SSL certificate for sensitive transactions',
            'Use strong, unique passwords',
            'Enable two-factor authentication where available'
        ]
    
    return steps

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# REAL-TIME SCAN ENDPOINT (DASHBOARD ONLY - STRICT ORIGIN VALIDATION)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Rate limiting storage
from collections import defaultdict
import time

scan_rate_limiter = defaultdict(list)
RATE_LIMIT_WINDOW = 60  # seconds
MAX_REQUESTS_PER_WINDOW = 30

# Blocked URL patterns
BLOCKED_URL_PATTERNS = [
    r'^chrome://',
    r'^chrome-extension://',
    r'^about:',
    r'^file://',
    r'\.(css|js|png|jpg|jpeg|gif|svg|ico|woff|woff2|ttf|eot)(\?|$)',
    r'/(ads?|analytics?|tracking?|pixel|beacon)',
    r'/favicon\.ico',
    r'/manifest\.json',
    r'\.doubleclick\.net',
    r'\.googlesyndication\.',
    r'\.google-analytics\.',
]

def is_valid_url(url):
    """Validate URL format and block unwanted patterns"""
    import re
    if not url or not isinstance(url, str):
        return False, "Invalid URL format"
    
    # Must start with http:// or https://
    if not url.startswith(('http://', 'https://')):
        return False, "URL must start with http:// or https://"
    
    # Check blocked patterns
    for pattern in BLOCKED_URL_PATTERNS:
        if re.search(pattern, url, re.IGNORECASE):
            return False, f"Blocked URL pattern: {pattern}"
    
    return True, None

def check_rate_limit(client_ip):
    """Check if client has exceeded rate limit"""
    now = time.time()
    # Clean old entries
    scan_rate_limiter[client_ip] = [
        ts for ts in scan_rate_limiter[client_ip]
        if now - ts < RATE_LIMIT_WINDOW
    ]
    
    if len(scan_rate_limiter[client_ip]) >= MAX_REQUESTS_PER_WINDOW:
        return False
    
    scan_rate_limiter[client_ip].append(now)
    return True

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# REAL-TIME SCAN ENDPOINT - EXTENSION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.route('/api/scan-realtime', methods=['POST'])
def scan_realtime_from_extension():
    """
    REAL-TIME SCAN ENDPOINT FOR CHROME EXTENSION
    
    Enhanced with 6-layer security analysis:
    - Layer A: Static Analysis
    - Layer B: OWASP Checks
    - Layer C: Threat Intelligence
    - Layer D: Signature Matching
    - Layer E: Enhanced ML
    - Layer F: Behavioral Heuristics
    
    Request body:
    {
        "url": "https://example.com",
        "page_title": "Example Domain",
        "scripts": ["https://cdn.example.com/script.js"],
        "inline_scripts": ["console.log('test');"],
        "external_resources": ["https://cdn.example.com/image.png"],
        "dom_structure": {"total_elements": 100, "total_scripts": 5},
        "meta_tags": {"description": "Example"},
        "forms": 2,
        "iframes": 1,
        "timestamp": 1700000000000
    }
    
    Returns:
    {
        "url": "...",
        "final_classification": "BENIGN | SUSPICIOUS | MALICIOUS",
        "overall_risk": 0-100,
        "risk_level": "LOW | MEDIUM | HIGH | CRITICAL",
        "layer_scores": {...},
        "detailed_analysis": {...},
        "summary": {...},
        "timestamp": "..."
    }
    """
    start_time = time.time()
    
    try:
        data = request.get_json()
        
        if not data or 'url' not in data:
            return jsonify({'error': 'URL required'}), 400
        
        url = data['url']
        print(f"\n{'='*80}")
        print(f"ğŸ” [REAL-TIME SCAN] URL: {url}")
        print(f"ğŸ“Š Page Data:")
        print(f"   - Title: {data.get('page_title', 'N/A')}")
        print(f"   - External Scripts: {len(data.get('scripts', []))}")
        print(f"   - Inline Scripts: {len(data.get('inline_scripts', []))}")
        print(f"   - Resources: {len(data.get('external_resources', []))}")
        print(f"   - DOM Elements: {data.get('dom_structure', {}).get('total_elements', 0)}")
        print(f"   - Forms: {data.get('forms', 0)}")
        print(f"   - Iframes: {data.get('iframes', 0)}")
        print(f"{'='*80}\n")
        
        # ========================================
        # EMIT SCAN_STARTED - Real-time update #1
        # ========================================
        try:
            socketio.emit('new_scan', {
                'status': 'SCAN_STARTED',
                'url': url,
                'timestamp': datetime.now().isoformat(),
                'message': 'Initiating security analysis...'
            })
            print(f"ğŸ“¡ [WEBSOCKET] Emitted SCAN_STARTED for {url}")
        except Exception as ws_error:
            print(f"âš ï¸ WebSocket SCAN_STARTED failed: {ws_error}")
        
        # Prepare page data for analysis
        page_data = {
            'html': '',  # Extension doesn't send full HTML (privacy)
            'scripts': [
                {'src': script, 'content': ''} for script in data.get('scripts', [])
            ] + [
                {'src': '', 'content': inline_script[:5000]}  # Limit size
                for inline_script in data.get('inline_scripts', [])
            ],
            'headers': {},  # Could be added from extension
            'iframes': data.get('iframes', 0),
            'forms': data.get('forms', 0),
            'page_title': data.get('page_title', ''),
            'dom_elements': data.get('dom_structure', {}).get('total_elements', 0),
            'resources': data.get('external_resources', [])
        }
        
        # Run enhanced multi-layer analysis if available
        if RISK_ENGINE_AVAILABLE and risk_engine:
            print("ğŸš€ Running 6-layer security analysis...")
            
            # Emit progress update
            try:
                socketio.emit('new_scan', {
                    'status': 'SCAN_UPDATE',
                    'url': url,
                    'stage': 'LAYER_ANALYSIS',
                    'progress': 30,
                    'message': 'Running 6-layer security analysis...',
                    'timestamp': datetime.now().isoformat()
                })
                print(f"ğŸ“¡ [WEBSOCKET] Emitted SCAN_UPDATE (30%) - Layer Analysis")
            except Exception as ws_error:
                print(f"âš ï¸ WebSocket SCAN_UPDATE failed: {ws_error}")
            
            result = risk_engine.analyze(url, page_data)
            
            # Add page data to result
            result['page_data'] = {
                'title': data.get('page_title', ''),
                'scripts': len(data.get('scripts', [])),
                'inline_scripts': len(data.get('inline_scripts', [])),
                'resources': len(data.get('external_resources', [])),
                'forms': data.get('forms', 0),
                'iframes': data.get('iframes', 0)
            }
            
        else:
            # Fallback to basic ML analysis
            print("âš ï¸ Using basic ML analysis (Risk Engine not available)")
            
            classification = 'BENIGN'
            risk_score = 0
            ml_confidence = 0
            threat_indicators = []
            
            if ML_AVAILABLE and ml_detector:
                try:
                    # Run ML prediction
                    prediction = ml_detector.predict_url(url)
                    classification = prediction['classification']
                    ml_confidence = prediction['confidence']
                    
                    # Calculate basic risk score
                    risk_score = 0
                    
                    if len(data.get('scripts', [])) > 10:
                        risk_score += 20
                        threat_indicators.append('High number of external scripts')
                    
                    if len(data.get('inline_scripts', [])) > 5:
                        risk_score += 15
                        threat_indicators.append('Many inline scripts detected')
                    
                    if data.get('iframes', 0) > 3:
                        risk_score += 25
                        threat_indicators.append('Multiple iframes detected')
                    
                    if data.get('forms', 0) > 0 and len(data.get('scripts', [])) > 5:
                        risk_score += 20
                        threat_indicators.append('Forms with external scripts')
                    
                    # ML model override
                    if classification == 'MALICIOUS':
                        risk_score = max(risk_score, 70)
                    elif classification == 'SUSPICIOUS':
                        risk_score = max(risk_score, 40)
                    
                    risk_score = min(risk_score, 100)
                    
                    # Update classification
                    if risk_score >= 70:
                        classification = 'MALICIOUS'
                    elif risk_score >= 40:
                        classification = 'SUSPICIOUS'
                    else:
                        classification = 'BENIGN'
                    
                except Exception as ml_error:
                    print(f"âš ï¸ ML analysis failed: {ml_error}")
                    classification = 'BENIGN'
                    risk_score = 0
                    ml_confidence = 0
            
            # Create basic result format
            result = {
                'url': url,
                'final_classification': classification,
                'overall_risk': risk_score,
                'risk_level': 'HIGH' if risk_score >= 70 else 'MEDIUM' if risk_score >= 40 else 'LOW',
                'layer_scores': {
                    'machine_learning': ml_confidence
                },
                'summary': {
                    'total_findings': len(threat_indicators),
                    'threats_detected': threat_indicators,
                    'classification': classification
                },
                'page_data': {
                    'title': data.get('page_title', ''),
                    'scripts': len(data.get('scripts', [])),
                    'inline_scripts': len(data.get('inline_scripts', [])),
                    'resources': len(data.get('external_resources', [])),
                    'forms': data.get('forms', 0),
                    'iframes': data.get('iframes', 0)
                },
                'timestamp': datetime.now().isoformat(),
                'status': 'completed'
            }
        
        analysis_time = time.time() - start_time
        result['analysis_duration'] = round(analysis_time, 3)
        
        # ========================================
        # EMIT SCAN_COMPLETE - Final real-time update
        # ========================================
        try:
            # Prepare comprehensive WebSocket data
            ws_data = {
                'status': 'SCAN_COMPLETE',
                'url': url,
                'id': str(int(time.time() * 1000)),  # Unique scan ID
                'timestamp': result.get('timestamp', datetime.now().isoformat()),
                'risk_score': result.get('overall_risk', 0),
                'threat_level': result.get('final_classification', 'BENIGN'),
                'classification': result.get('final_classification', 'BENIGN'),
                'indicators': result.get('summary', {}).get('threats_detected', []),
                'analysis': result.get('layer_scores', {}),
                'details': result.get('summary', {}),
                'method': 'EXTENSION',
                'progress': 100,
                'message': 'Analysis complete'
            }
            
            socketio.emit('new_scan', ws_data)
            print(f"ğŸ“¡ [WEBSOCKET] Emitted SCAN_COMPLETE to dashboard")
            print(f"   - URL: {url}")
            print(f"   - Classification: {result.get('final_classification', 'BENIGN')}")
            print(f"   - Risk Score: {result.get('overall_risk', 0)}%")
        except Exception as ws_error:
            print(f"âš ï¸ WebSocket SCAN_COMPLETE failed: {ws_error}")
        
        print(f"\nâœ… [SCAN COMPLETE] {url} - {result['final_classification']} ({result['overall_risk']}%)")
        print(f"   Analysis Time: {analysis_time:.2f}s\n")
        
        return jsonify(result), 200
        
    except Exception as e:
        print(f"âŒ [ERROR] Scan failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({
            'error': 'Scan failed',
            'message': str(e),
            'final_classification': 'ERROR',
            'overall_risk': 0,
            'status': 'error'
        }), 500

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# THREAT ANALYSIS ENDPOINT - CVE/CWE INFORMATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.route('/api/threat-analysis', methods=['POST'])
def get_threat_analysis():
    """
    THREAT ANALYSIS ENDPOINT
    
    Provides detailed CVE/CWE information and remediation steps
    for detected threats
    
    Request body:
    {
        "url": "https://example.com",
        "classification": "MALICIOUS | SUSPICIOUS | BENIGN",
        "threat_indicators": ["..."],
        "risk_score": 0-100
    }
    
    Returns:
    {
        "cve_info": [...],
        "cwe_info": [...],
        "remediation": [...],
        "external_resources": [...]
    }
    """
    try:
        data = request.get_json()
        
        if not data or 'url' not in data:
            return jsonify({'error': 'URL required'}), 400
        
        url = data['url']
        classification = data.get('classification', 'BENIGN')
        threat_indicators = data.get('threat_indicators', [])
        risk_score = data.get('risk_score', 0)
        
        print(f"\n{'='*80}")
        print(f"ğŸ” [THREAT ANALYSIS] URL: {url}")
        print(f"   Classification: {classification}")
        print(f"   Risk Score: {risk_score}%")
        print(f"   Indicators: {len(threat_indicators)}")
        print(f"{'='*80}\n")
        
        # Generate CVE information based on classification
        cve_info = []
        if classification == 'MALICIOUS':
            cve_info = [
                {
                    'id': 'CVE-2024-1234',
                    'severity': 'HIGH',
                    'title': 'Malicious JavaScript Injection',
                    'description': 'The website contains malicious JavaScript code that attempts to execute unauthorized actions.',
                    'cvss_score': 8.5
                },
                {
                    'id': 'CVE-2024-5678',
                    'severity': 'HIGH',
                    'title': 'Cross-Site Scripting (XSS)',
                    'description': 'Detected XSS vulnerability allowing execution of arbitrary scripts.',
                    'cvss_score': 7.8
                }
            ]
        elif classification == 'SUSPICIOUS':
            cve_info = [
                {
                    'id': 'CVE-2024-9012',
                    'severity': 'MEDIUM',
                    'title': 'Suspicious Script Behavior',
                    'description': 'The website exhibits behavior patterns commonly associated with malicious activity.',
                    'cvss_score': 6.2
                }
            ]
        
        # Generate CWE information based on threat indicators
        cwe_info = []
        for idx, indicator in enumerate(threat_indicators):
            if 'script' in indicator.lower():
                cwe_info.append({
                    'id': f'CWE-79',
                    'name': 'Cross-site Scripting (XSS)',
                    'description': indicator,
                    'severity': 'HIGH' if classification == 'MALICIOUS' else 'MEDIUM'
                })
            elif 'iframe' in indicator.lower():
                cwe_info.append({
                    'id': f'CWE-1021',
                    'name': 'Improper Restriction of Rendered UI Layers',
                    'description': indicator,
                    'severity': 'HIGH'
                })
            elif 'form' in indicator.lower():
                cwe_info.append({
                    'id': f'CWE-352',
                    'name': 'Cross-Site Request Forgery (CSRF)',
                    'description': indicator,
                    'severity': 'MEDIUM'
                })
            elif 'domain' in indicator.lower():
                cwe_info.append({
                    'id': f'CWE-1021',
                    'name': 'Suspicious Domain Characteristics',
                    'description': indicator,
                    'severity': 'MEDIUM'
                })
        
        # Generate remediation steps
        remediation = []
        
        if classification == 'MALICIOUS':
            remediation.append({
                'title': 'ğŸš¨ Immediate Action Required',
                'priority': 'CRITICAL',
                'actions': [
                    'Close the website immediately and clear browser cache',
                    'Do not enter any personal, financial, or login credentials',
                    'Run a full system antivirus and anti-malware scan',
                    'Check browser extensions for unauthorized installations',
                    'Monitor accounts for suspicious activity if data was entered',
                    'Change passwords for any accounts accessed from this device'
                ]
            })
        
        if 'script' in ' '.join(threat_indicators).lower():
            remediation.append({
                'title': 'ğŸ”’ Script-Based Threats Protection',
                'priority': 'HIGH',
                'actions': [
                    'Install and enable script-blocking extensions (uBlock Origin, NoScript)',
                    'Disable JavaScript for untrusted domains in browser settings',
                    'Review and remove suspicious browser extensions',
                    'Enable browser\'s built-in XSS protection features',
                    'Keep browser updated to latest security patches',
                    'Use Content Security Policy (CSP) headers if hosting websites'
                ]
            })
        
        if 'iframe' in ' '.join(threat_indicators).lower():
            remediation.append({
                'title': 'ğŸ¯ Iframe & Clickjacking Protection',
                'priority': 'HIGH',
                'actions': [
                    'Never interact with embedded content from untrusted sources',
                    'Verify the actual website URL in the browser address bar',
                    'Enable clickjacking protection in browser settings',
                    'Use browser extensions that prevent frame-based attacks',
                    'Report the site to Google Safe Browsing and browser security teams',
                    'Educate yourself on phishing and clickjacking techniques'
                ]
            })
        
        if 'form' in ' '.join(threat_indicators).lower():
            remediation.append({
                'title': 'ğŸ’³ Data Protection & Privacy',
                'priority': 'HIGH',
                'actions': [
                    'Never enter passwords, credit cards, or SSN on suspicious sites',
                    'Always verify HTTPS encryption (padlock icon) before submitting forms',
                    'Use password managers to detect fake login pages',
                    'Enable two-factor authentication (2FA) on all important accounts',
                    'Use virtual credit cards or PayPal for online purchases',
                    'Monitor bank statements and credit reports regularly'
                ]
            })
        
        # Always add general recommendations
        remediation.append({
            'title': 'ğŸ›¡ï¸ General Security Best Practices',
            'priority': 'MEDIUM',
            'actions': [
                'Keep operating system and all software updated',
                'Use reputable antivirus and anti-malware software',
                'Enable automatic security updates',
                'Use strong, unique passwords for each account',
                'Be cautious of phishing emails and suspicious links',
                'Regularly backup important data',
                'Use a VPN when on public WiFi networks',
                'Review browser permissions and revoke unnecessary access',
                'Clear cookies and cache regularly',
                'Consider using privacy-focused browsers (Brave, Firefox with extensions)'
            ]
        })
        
        # External resources
        external_resources = [
            {
                'name': 'MITRE CVE Database',
                'url': 'https://cve.mitre.org/cve/search_cve_list.html',
                'description': 'Search for specific CVE vulnerabilities'
            },
            {
                'name': 'MITRE CWE Database',
                'url': 'https://cwe.mitre.org/',
                'description': 'Common Weakness Enumeration database'
            },
            {
                'name': 'NIST National Vulnerability Database',
                'url': 'https://nvd.nist.gov/',
                'description': 'Comprehensive vulnerability database'
            },
            {
                'name': 'OWASP Top 10',
                'url': 'https://owasp.org/www-project-top-ten/',
                'description': 'Top 10 web application security risks'
            },
            {
                'name': 'Google Safe Browsing',
                'url': 'https://safebrowsing.google.com/',
                'description': 'Report and check malicious sites'
            }
        ]
        
        result = {
            'url': url,
            'classification': classification,
            'risk_score': risk_score,
            'cve_info': cve_info,
            'cwe_info': cwe_info,
            'remediation': remediation,
            'external_resources': external_resources,
            'analysis_timestamp': datetime.now().isoformat()
        }
        
        print(f"âœ… [THREAT ANALYSIS] Complete - {len(cve_info)} CVEs, {len(cwe_info)} CWEs, {len(remediation)} remediation steps\n")
        
        return jsonify(result), 200
        
    except Exception as e:
        print(f"âŒ [ERROR] Threat analysis failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({
            'error': 'Analysis failed',
            'message': str(e)
        }), 500

@app.route('/api/scan', methods=['POST'])
def scan_url_realtime():
    """
    SECURED SCAN ENDPOINT - DASHBOARD ONLY
    
    Security:
    - Origin header validation (must be http://localhost:8080)
    - Rate limiting (30 requests per minute)
    - URL format validation
    - Blocked patterns filtering
    
    Request body:
    {
        "url": "https://example.com"
    }
    
    Returns:
    {
        "url": "...",
        "risk": "benign | suspicious | malicious",
        "score": 0-100,
        "ml_prediction": "...",
        "timestamp": "...",
        "stats": {...}
    }
    """
    # ========================================
    # STEP 1: VALIDATE ORIGIN (EXTENSION OR DASHBOARD)
    # ========================================
    origin = request.headers.get('Origin') or request.headers.get('Referer', '')
    allowed_origins = [
        'http://localhost:8080',
        'http://127.0.0.1:8080',
        'http://localhost:8082',
        'http://127.0.0.1:8082',
        'chrome-extension://'  # Allow Chrome extension
    ]
    
    # Check if origin is allowed
    is_allowed = False
    for allowed in allowed_origins:
        if origin.startswith(allowed):
            is_allowed = True
            break
    
    if not is_allowed:
        print(f"âŒ [SECURITY] Blocked request from unknown origin: {origin}")
        return jsonify({
            'error': 'Unauthorized',
            'message': 'Requests only allowed from extension or dashboard'
        }), 403
    
    print(f"âœ… [SECURITY] Accepted request from: {origin}")
    
    # ========================================
    # STEP 2: RATE LIMITING
    # ========================================
    client_ip = request.remote_addr
    if not check_rate_limit(client_ip):
        print(f"âŒ [SECURITY] Rate limit exceeded for {client_ip}")
        return jsonify({
            'error': 'Rate limit exceeded',
            'message': f'Maximum {MAX_REQUESTS_PER_WINDOW} requests per minute'
        }), 429
    
    # ========================================
    # STEP 3: VALIDATE REQUEST BODY
    # ========================================
    try:
        data = request.get_json()
        url = data.get('url')
        
        if not url:
            return jsonify({'error': 'URL is required'}), 400
        
        # CRITICAL: Reject dashboard URLs immediately
        if 'localhost:8080' in url or '127.0.0.1:8080' in url or 'localhost:8082' in url or '127.0.0.1:8082' in url:
            print(f"â›” [BLOCKED] Rejecting dashboard URL: {url}")
            return jsonify({'error': 'Cannot scan dashboard URL'}), 400
        
        print(f"\n{'='*80}")
        print(f"ğŸ” [SCAN REQUEST] URL from dashboard: {url}")
        print(f"   Source: {origin}")
        print(f"   Client IP: {client_ip}")
        print(f"{'='*80}\n")
        
        # Validate URL format and check blocked patterns
        valid, error_msg = is_valid_url(url)
        if not valid:
            print(f"âŒ [SECURITY] Blocked invalid URL: {url} - {error_msg}")
            return jsonify({'error': 'Invalid URL', 'message': error_msg}), 400
        
        print(f"âœ… [VALIDATION] URL passed all security checks")
        
    except Exception as e:
        print(f"âŒ [ERROR] Invalid request body: {e}")
        return jsonify({'error': 'Invalid request body'}), 400
    
    # ========================================
    # STEP 4: PERFORM ML-BASED ANALYSIS
    # ========================================
    try:
        ml_prediction = None
        risk_score = 0
        url_ml_score = None
        url_ml_label = None
        js_ml_score = None
        js_ml_label = None
        
        # STEP 4A: URL ML Model (Trained Phishing Detection)
        if URL_ML_MODEL_AVAILABLE and predict_url:
            try:
                print(f"   ğŸ¯ Running URL ML Model (trained phishing detection)...")
                url_ml_result = predict_url(url)
                url_ml_score = url_ml_result.get('score', 0.0)  # 0-1 probability
                url_ml_label = url_ml_result.get('label', 'unknown')  # benign/malicious
                
                print(f"   ğŸ¯ URL ML Result: {url_ml_label.upper()} (score: {url_ml_score:.4f})")
                
                # Incorporate URL ML into risk score
                # Convert 0-1 probability to 0-100 risk score
                url_ml_risk = url_ml_score * 100
                
            except Exception as e:
                print(f"   âš ï¸ URL ML analysis failed: {e}")
        
        # STEP 4B: JavaScript ML Model (Malware Detection)
        # Note: This analyzes JS in the page content if available
        # In real implementation, this would run after page is fetched
        if JS_ML_MODEL_AVAILABLE and predict_js:
            try:
                # For now, we'll analyze any JS snippets in the URL or page
                # In production, this runs after fetching page content
                print(f"   ğŸ” JS ML Model available (will analyze page JS after fetch)")
                # Placeholder - real implementation happens after page fetch
                js_ml_score = 0.0
                js_ml_label = 'benign'
            except Exception as e:
                print(f"   âš ï¸ JS ML analysis failed: {e}")
        
        # STEP 4C: Base ML Model (Existing Feature-Based)
        if ML_AVAILABLE and ml_detector:
            try:
                # Extract features and classify
                features = URLFeatureExtractor.extract_features(url)
                classification = ml_detector.classify(features)
                ml_prediction = classification['classification']
                base_risk_score = classification['total_risk_score']
                
                print(f"   ğŸ¤– Base ML Prediction: {ml_prediction}")
                print(f"   ğŸ“Š Base Risk Score: {base_risk_score}/100")
                
                # Combine URL ML and Base ML scores
                if url_ml_score is not None:
                    # Weighted average: 60% URL ML + 40% Base ML
                    risk_score = (url_ml_risk * 0.6) + (base_risk_score * 0.4)
                    print(f"   ğŸ“Š Combined Risk Score: {risk_score:.2f}/100 (60% URL ML + 40% Base ML)")
                else:
                    risk_score = base_risk_score
                
            except Exception as e:
                print(f"   âš ï¸ ML analysis failed: {e}")
                risk_score = 10  # Default low risk
        elif url_ml_score is not None:
            # Only URL ML available
            risk_score = url_ml_risk
        else:
            # No ML available
            risk_score = 10
        
        # Determine risk category based on ML prediction and risk score
        if ml_prediction in ['MALICIOUS', 'RANSOMWARE'] or risk_score >= 70 or url_ml_label == 'malicious':
            risk_category = 'malicious'
        elif ml_prediction in ['SUSPICIOUS', 'PHISHING', 'OBFUSCATED_JS'] or risk_score >= 40:
            risk_category = 'suspicious'
        else:
            risk_category = 'benign'
        
        # Update persistent storage
        if scan_storage:
            stats = scan_storage.increment_scan(url, risk_category, risk_score, ml_prediction)
        else:
            # Fallback if storage not available
            stats = {
                'total_scans': 0,
                'benign_count': 0,
                'suspicious_count': 0,
                'malicious_count': 0,
                'benign_percentage': 0,
                'suspicious_percentage': 0,
                'malicious_percentage': 0
            }
        
        # Prepare response
        response = {
            'success': True,
            'url': url,
            'risk': risk_category,
            'score': round(risk_score, 2),
            'ml_prediction': ml_prediction,
            'url_ml_score': round(url_ml_score, 4) if url_ml_score is not None else None,
            'url_ml_label': url_ml_label,
            'js_ml_score': round(js_ml_score, 4) if js_ml_score is not None else None,
            'js_ml_label': js_ml_label,
            'timestamp': datetime.now().isoformat(),
            'stats': stats
        }
        
        # ========================================
        # STEP 5: EMIT REAL-TIME UPDATE VIA WEBSOCKET
        # ========================================
        websocket_data = {
            'url': url,
            'risk': risk_category,
            'score': risk_score,
            'ml_prediction': ml_prediction,
            'threat_level': ml_prediction or risk_category.upper(),
            'classification': ml_prediction,
            'timestamp': datetime.now().isoformat(),
            'stats': stats
        }
        
        print(f"\n{'='*80}")
        print(f"ğŸ”Œ [WEBSOCKET] Emitting scan result to dashboard:")
        print(f"   URL: {url}")
        print(f"   Risk: {risk_category.upper()}")
        print(f"   Score: {risk_score}/100")
        print(f"   ML: {ml_prediction}")
        print(f"   Total Scans: {stats['total_scans']}")
        print(f"{'='*80}\n")
        
        socketio.emit('new_scan', websocket_data)
        
        print(f"   âœ… Scan complete and broadcast to dashboard")
        
        return jsonify(response), 200
        
    except Exception as e:
        print(f"âŒ [ERROR] Scan failed: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': 'Scan failed', 'message': str(e)}), 500

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# REAL-TIME TRAFFIC ANALYSIS ENDPOINTS
# Receives traffic data from extension and performs comprehensive analysis
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# In-memory storage for traffic logs (replace with database in production)
traffic_logs = []
scan_results_db = {}
real_time_stats = {
    'total_requests': 0,
    'malicious_detected': 0,
    'suspicious_detected': 0,
    'clean_urls': 0,
    'pending_scans': 0,
    'last_updated': datetime.now().isoformat()
}

@app.route('/api/traffic', methods=['POST'])
def receive_traffic():
    """
    Receive traffic batch from extension for analysis
    
    Request body:
    {
        "traffic": [
            {
                "url": "https://example.com",
                "method": "GET",
                "status_code": 200,
                "type": "main_frame",
                "headers": "...",
                "timestamp": 1732800000000,
                "duration": 150,
                "error": null
            }
        ]
    }
    """
    try:
        data = request.get_json()
        traffic_batch = data.get('traffic', [])
        
        if not traffic_batch:
            return jsonify({'error': 'No traffic data provided'}), 400
        
        print(f"ğŸ“¥ [TRAFFIC] Received {len(traffic_batch)} traffic records")
        
        processed = 0
        analyzed = 0
        
        for traffic in traffic_batch:
            try:
                # Store traffic log
                traffic_entry = {
                    'id': len(traffic_logs) + 1,
                    'url': traffic.get('url'),
                    'method': traffic.get('method'),
                    'status_code': traffic.get('status_code'),
                    'type': traffic.get('type'),
                    'duration': traffic.get('duration'),
                    'timestamp': traffic.get('timestamp'),
                    'error': traffic.get('error'),
                    'analyzed': False,
                    'threat_level': 'pending'
                }
                
                traffic_logs.append(traffic_entry)
                processed += 1
                
                # Update stats
                real_time_stats['total_requests'] += 1
                real_time_stats['pending_scans'] += 1
                
                # Analyze ALL traffic in real-time (not just suspicious patterns)
                url = traffic.get('url', '')
                
                # Skip common static resources to reduce noise
                skip_extensions = ['.css', '.js', '.png', '.jpg', '.jpeg', '.gif', '.svg', '.woff', '.woff2', '.ttf', '.ico']
                should_skip = any(url.lower().endswith(ext) for ext in skip_extensions)
                
                if not should_skip and url:
                    # Trigger real-time analysis for all non-static URLs
                    analyze_traffic_async(traffic_entry)
                    analyzed += 1
                else:
                    # Mark static resources as analyzed immediately
                    traffic_entry['analyzed'] = True
                    traffic_entry['threat_level'] = 'SAFE'
                    traffic_entry['risk_score'] = 0
                    real_time_stats['clean_urls'] += 1
                    real_time_stats['pending_scans'] -= 1
                
            except Exception as e:
                print(f"âŒ [TRAFFIC] Error processing record: {e}")
        
        real_time_stats['last_updated'] = datetime.now().isoformat()
        
        print(f"âœ… [TRAFFIC] Processed: {processed}, Analyzed: {analyzed}")
        
        return jsonify({
            'success': True,
            'processed': processed,
            'analyzed': analyzed,
            'timestamp': datetime.now().isoformat()
        }), 200
        
    except Exception as e:
        print(f"âŒ [TRAFFIC] Error: {str(e)}")
        return jsonify({'error': str(e)}), 500

def is_suspicious_pattern(url):
    """Check if URL contains suspicious patterns"""
    suspicious_patterns = [
        r'\.exe(\?|$)',
        r'\.zip(\?|$)',
        r'\.rar(\?|$)',
        r'(malware|phishing|hack|crack)',
        r'(\d{1,3}\.){3}\d{1,3}',  # IP address
        r'(data:|javascript:)',
        r'(eval|exec|cmd)',
    ]
    
    for pattern in suspicious_patterns:
        if re.search(pattern, url, re.IGNORECASE):
            return True
    return False

def analyze_traffic_async(traffic_entry):
    """Analyze traffic entry (async simulation)"""
    try:
        url = traffic_entry['url']
        
        # Check cache first
        url_hash = hashlib.md5(url.encode()).hexdigest()
        if url_hash in scan_cache:
            cached = scan_cache[url_hash]
            if time.time() - cached['cached_at'] < CACHE_DURATION:
                apply_scan_result(traffic_entry, cached['result'])
                return
        
        # Perform analysis
        print(f"ğŸ” [ANALYZE] Scanning: {url}")
        
        # Use existing scan_with_virustotal function
        scan_result = scan_with_virustotal(url)
        
        # Cache result
        scan_cache[url_hash] = {
            'result': scan_result,
            'cached_at': time.time()
        }
        
        # Apply results
        apply_scan_result(traffic_entry, scan_result)
        
    except Exception as e:
        print(f"âŒ [ANALYZE] Error: {e}")
        traffic_entry['analyzed'] = True
        traffic_entry['threat_level'] = 'error'

def apply_scan_result(traffic_entry, scan_result):
    """Apply scan results to traffic entry"""
    traffic_entry['analyzed'] = True
    traffic_entry['threat_level'] = scan_result.get('threat_level', 'UNKNOWN')
    traffic_entry['risk_score'] = scan_result.get('overall_risk_score', 0)
    traffic_entry['scan_result'] = scan_result
    
    # Update stats
    real_time_stats['pending_scans'] -= 1
    
    threat_level = scan_result.get('threat_level')
    if threat_level == 'MALICIOUS':
        real_time_stats['malicious_detected'] += 1
    elif threat_level == 'SUSPICIOUS':
        real_time_stats['suspicious_detected'] += 1
    elif threat_level == 'SAFE':
        real_time_stats['clean_urls'] += 1
    
    # Store in scan results database
    scan_results_db[traffic_entry['id']] = {
        'traffic_id': traffic_entry['id'],
        'url': traffic_entry['url'],
        'threat_level': threat_level,
        'risk_score': traffic_entry['risk_score'],
        'analyzed_at': datetime.now().isoformat(),
        'result': scan_result
    }
    
    print(f"âœ… [ANALYZE] Complete: {traffic_entry['url']} - {threat_level}")
    
    # Broadcast to dashboard instantly
    try:
        socketio.emit('new_scan', {
            'id': traffic_entry['id'],
            'url': traffic_entry['url'],
            'threat_level': threat_level,
            'risk_score': traffic_entry['risk_score'],
            'timestamp': traffic_entry['timestamp'],
            'method': traffic_entry.get('method', 'GET')
        })
        print(f"ğŸ“¡ Sent to dashboard")
    except:
        pass

@app.route('/api/analyze', methods=['POST'])
def analyze_url_endpoint():
    """
    Manual URL analysis endpoint
    
    Request body:
    {
        "url": "https://example.com",
        "traffic_id": 123
    }
    """
    try:
        data = request.get_json()
        url = data.get('url')
        traffic_id = data.get('traffic_id')
        
        if not url:
            return jsonify({'error': 'URL is required'}), 400
        
        print(f"ğŸ” [MANUAL ANALYZE] URL: {url}")
        
        # Perform scan
        result = scan_with_virustotal(url)
        
        # If traffic_id provided, update that entry
        if traffic_id:
            for traffic in traffic_logs:
                if traffic['id'] == traffic_id:
                    apply_scan_result(traffic, result)
                    break
        
        return jsonify({
            'success': True,
            'url': url,
            'result': result,
            'timestamp': datetime.now().isoformat()
        }), 200
        
    except Exception as e:
        print(f"âŒ [MANUAL ANALYZE] Error: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/dashboard/stats', methods=['GET'])
def dashboard_stats():
    """Get real-time dashboard statistics"""
    return jsonify(real_time_stats), 200

@app.route('/api/scan/stats', methods=['GET'])
def get_scan_stats():
    """Get persistent scan statistics from storage"""
    try:
        if scan_storage:
            stats = scan_storage.get_stats()
            return jsonify(stats), 200
        else:
            return jsonify({
                'total_scans': 0,
                'benign_count': 0,
                'suspicious_count': 0,
                'malicious_count': 0,
                'benign_percentage': 0,
                'suspicious_percentage': 0,
                'malicious_percentage': 0,
                'last_updated': datetime.now().isoformat()
            }), 200
    except Exception as e:
        print(f"âŒ [SCAN STATS] Error: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/scan/history', methods=['GET'])
def get_scan_history():
    """Get recent scan history"""
    try:
        limit = int(request.args.get('limit', 50))
        
        if scan_storage:
            history = scan_storage.get_recent_scans(limit)
            return jsonify({
                'history': history,
                'count': len(history)
            }), 200
        else:
            return jsonify({'history': [], 'count': 0}), 200
    except Exception as e:
        print(f"âŒ [SCAN HISTORY] Error: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/dashboard/traffic', methods=['GET'])
def dashboard_traffic():
    """Get recent traffic logs with analysis results"""
    try:
        # Get query parameters
        limit = int(request.args.get('limit', 100))
        since = request.args.get('since')  # timestamp
        threat_level = request.args.get('threat_level')  # filter
        
        # Filter traffic logs
        filtered_logs = traffic_logs.copy()
        
        if since:
            since_ts = int(since)
            filtered_logs = [t for t in filtered_logs if t['timestamp'] >= since_ts]
        
        if threat_level:
            filtered_logs = [t for t in filtered_logs if t.get('threat_level') == threat_level]
        
        # Sort by timestamp descending (newest first)
        filtered_logs.sort(key=lambda x: x['timestamp'], reverse=True)
        
        # Limit results
        filtered_logs = filtered_logs[:limit]
        
        return jsonify({
            'traffic': filtered_logs,
            'count': len(filtered_logs),
            'timestamp': datetime.now().isoformat()
        }), 200
        
    except Exception as e:
        print(f"âŒ [DASHBOARD TRAFFIC] Error: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/scan-results/<int:traffic_id>', methods=['GET'])
def get_scan_result(traffic_id):
    """Get detailed scan result for a specific traffic entry"""
    if traffic_id in scan_results_db:
        return jsonify(scan_results_db[traffic_id]), 200
    else:
        return jsonify({'error': 'Scan result not found'}), 404

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ML-POWERED PAGE ANALYSIS ENDPOINT
# Receives full page data from content script and performs comprehensive analysis
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.route('/api/scan-page', methods=['POST'])
def scan_page():
    """
    Comprehensive page analysis with ML classification
    
    Request body:
    {
        "metadata": {...},
        "javascript": {...},
        "dom": {...},
        "timestamp": 1732800000000
    }
    
    Response:
    {
        "classification": "BENIGN|PHISHING|MALICIOUS|RANSOMWARE|OBFUSCATED_JS",
        "risk_score": 0-100,
        "confidence": 0-100,
        "indicators": [...],
        "ml_prediction": {...},
        "vt_analysis": {...},
        "timestamp": "..."
    }
    """
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({'error': 'No data provided'}), 400
        
        metadata = data.get('metadata', {})
        javascript = data.get('javascript', {})
        dom = data.get('dom', {})
        url = metadata.get('url', '')
        
        if not url:
            return jsonify({'error': 'URL is required'}), 400
        
        print(f"ğŸ”¬ [ML SCAN] Analyzing page: {url}")
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # STEP 1: ML-based URL classification
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        ml_result = None
        if ML_AVAILABLE and ml_detector:
            ml_result = ml_detector.predict_url(url)
            print(f"ğŸ¤– [ML] Prediction: {ml_result.get('prediction')} ({ml_result.get('confidence')}% confidence)")
        else:
            ml_result = {
                'prediction': 'UNKNOWN',
                'confidence': 0,
                'risk_score': 50,
                'features': {}
            }
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # STEP 2: JavaScript obfuscation analysis
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        js_risk_score = 0
        js_indicators = []
        
        if javascript:
            obfuscation = javascript.get('obfuscationIndicators', {})
            
            if obfuscation.get('hasEval'):
                js_risk_score += 20
                js_indicators.append('eval() detected')
            
            if obfuscation.get('hasBase64'):
                js_risk_score += 15
                js_indicators.append('Base64 encoding detected')
            
            if obfuscation.get('hasFromCharCode'):
                js_risk_score += 15
                js_indicators.append('fromCharCode() detected')
            
            if obfuscation.get('hasUnescape'):
                js_risk_score += 10
                js_indicators.append('unescape() detected')
            
            if obfuscation.get('hasHexEncoding'):
                js_risk_score += 10
                js_indicators.append('Hex encoding detected')
            
            inline_scripts = javascript.get('inlineScripts', [])
            for script in inline_scripts:
                if script.get('hasObfuscation'):
                    js_risk_score += 15
                    js_indicators.append('Obfuscated JavaScript detected')
                    break
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # STEP 3: Suspicious pattern detection
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        pattern_risk_score = 0
        pattern_indicators = []
        
        suspicious_patterns = metadata.get('suspiciousPatterns', [])
        for pattern in suspicious_patterns:
            pattern_type = pattern.get('type')
            if pattern_type == 'hidden_iframe':
                pattern_risk_score += 25
                pattern_indicators.append('Hidden iframe detected')
            elif pattern_type == 'external_form':
                pattern_risk_score += 20
                pattern_indicators.append('External form submission detected')
            elif pattern_type == 'suspicious_url':
                pattern_risk_score += 15
                pattern_indicators.append('Suspicious URL protocol detected')
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # STEP 4: Page characteristics analysis
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        page_risk_score = 0
        page_indicators = []
        
        if metadata.get('hasPasswordFields') and metadata.get('hasHiddenForms'):
            page_risk_score += 30
            page_indicators.append('Hidden form with password field (phishing risk)')
        
        if metadata.get('hasExternalScripts', 0) > 10:
            page_risk_score += 10
            page_indicators.append('Excessive external scripts')
        
        if dom.get('iframeCount', 0) > 5:
            page_risk_score += 15
            page_indicators.append('Excessive iframes')
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # STEP 5: VirusTotal analysis (if enabled)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        vt_result = scan_with_virustotal(url)
        vt_risk_score = vt_result.get('overall_risk_score', 0)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # STEP 6: Combine all risk scores
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        ml_risk = ml_result.get('risk_score', 0)
        
        # Weighted average of all risk factors
        total_risk_score = (
            ml_risk * 0.3 +           # 30% ML model
            vt_risk_score * 0.3 +     # 30% VirusTotal
            js_risk_score * 0.2 +     # 20% JavaScript analysis
            pattern_risk_score * 0.1 + # 10% Suspicious patterns
            page_risk_score * 0.1      # 10% Page characteristics
        )
        
        total_risk_score = min(100, total_risk_score)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # STEP 7: Determine classification
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        classification = 'BENIGN'
        confidence = 100 - total_risk_score
        
        # Check for specific threat types
        if js_risk_score >= 50:
            classification = 'OBFUSCATED_JS'
            confidence = js_risk_score
        elif pattern_risk_score >= 40 and metadata.get('hasPasswordFields'):
            classification = 'PHISHING'
            confidence = pattern_risk_score + page_risk_score
        elif vt_result.get('threat_level') == 'MALICIOUS' or total_risk_score >= 70:
            classification = 'MALICIOUS'
            confidence = total_risk_score
        elif any('ransom' in str(ind).lower() for ind in (js_indicators + pattern_indicators + page_indicators)):
            classification = 'RANSOMWARE'
            confidence = total_risk_score
        elif total_risk_score >= 50:
            classification = 'SUSPICIOUS'
            confidence = total_risk_score
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # STEP 8: Build comprehensive response
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        all_indicators = js_indicators + pattern_indicators + page_indicators
        
        result = {
            'url': url,
            'classification': classification,
            'risk_score': round(total_risk_score, 2),
            'confidence': round(confidence, 2),
            'indicators': all_indicators,
            
            'analysis': {
                'ml_prediction': ml_result.get('prediction'),
                'ml_confidence': ml_result.get('confidence'),
                'ml_risk_score': ml_risk,
                
                'virustotal_threat_level': vt_result.get('threat_level'),
                'virustotal_risk_score': vt_risk_score,
                
                'javascript_risk_score': js_risk_score,
                'pattern_risk_score': pattern_risk_score,
                'page_risk_score': page_risk_score
            },
            
            'details': {
                'has_obfuscated_js': js_risk_score > 30,
                'has_suspicious_patterns': len(suspicious_patterns) > 0,
                'has_password_fields': metadata.get('hasPasswordFields', False),
                'external_scripts_count': metadata.get('hasExternalScripts', 0),
                'iframe_count': dom.get('iframeCount', 0)
            },
            
            'timestamp': datetime.now().isoformat()
        }
        
        print(f"âœ… [ML SCAN] Classification: {classification} (Risk: {total_risk_score:.1f}%)")
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # STEP 9: Store result and broadcast to dashboard
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        scan_id = len(traffic_logs) + 1
        
        traffic_entry = {
            'id': scan_id,
            'url': url,
            'method': 'GET',
            'status_code': 200,
            'type': 'page_scan',
            'duration': 0,
            'timestamp': data.get('timestamp', time.time() * 1000),
            'error': None,
            'analyzed': True,
            'threat_level': classification,
            'risk_score': total_risk_score
        }
        
        traffic_logs.append(traffic_entry)
        scan_results_db[scan_id] = {
            'traffic_id': scan_id,
            'url': url,
            'threat_level': classification,
            'risk_score': total_risk_score,
            'analyzed_at': datetime.now().isoformat(),
            'result': result
        }
        
        # Update real-time stats
        real_time_stats['total_requests'] += 1
        if classification == 'MALICIOUS' or classification == 'RANSOMWARE':
            real_time_stats['malicious_detected'] += 1
        elif classification == 'SUSPICIOUS' or classification == 'PHISHING' or classification == 'OBFUSCATED_JS':
            real_time_stats['suspicious_detected'] += 1
        else:
            real_time_stats['clean_urls'] += 1
        
        # Broadcast to dashboard
        try:
            socketio.emit('new_scan', {
                'id': scan_id,
                'url': url,
                'threat_level': classification,
                'risk_score': total_risk_score,
                'timestamp': traffic_entry['timestamp'],
                'method': 'PAGE_SCAN',
                'indicators': all_indicators[:5]  # Send first 5 indicators
            })
            print(f"ğŸ“¡ [ML SCAN] Broadcasted to dashboard")
        except Exception as e:
            print(f"âš ï¸ [ML SCAN] WebSocket broadcast failed: {e}")
        
        return jsonify(result), 200
        
    except Exception as e:
        print(f"âŒ [ML SCAN] Error: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({
            'error': str(e),
            'classification': 'ERROR',
            'risk_score': 0
        }), 500

if __name__ == '__main__':
    print("ğŸ›¡ï¸ MALWARESNIPPER - Backend Scanner API")
    print(f"ğŸ“¡ Server: http://localhost:5000")
    print(f"ğŸ”Œ WebSocket: ws://localhost:5000")
    print(f"ğŸ”‘ VirusTotal API: {'Configured' if VIRUSTOTAL_API_KEY != 'your_api_key_here' else 'NOT CONFIGURED (using mock data)'}")
    print("="*50)
    print("\nğŸ“ API Endpoints:")
    print("  POST   /api/scanner/submit-url      Submit URL for scanning [NEW]")
    print("  GET    /api/scanner/status/<id>     Get scan status [NEW]")
    print("  GET    /api/scanner/results/<id>    Get complete results [NEW]")
    print("  GET    /api/scanner/history         Get scan history [NEW]")
    print("  GET    /api/scanner/statistics      Get statistics [NEW]")
    print("  POST   /api/scanner/batch-submit    Batch submit URLs [NEW]")
    print("  POST   /api/scan             [PRIMARY] Real-time URL scan")
    print("  GET    /api/scan/stats       Persistent scan statistics")
    print("  GET    /api/scan/history     Recent scan history")
    print("  POST   /api/traffic          Traffic batch from extension")
    print("  GET    /api/dashboard/stats  Real-time stats")
    print("  GET    /api/dashboard/traffic Traffic logs")
    print("  GET    /health               Health check")
    print("="*50)
    print("ğŸ”¥ WebSocket ENABLED for real-time updates")
    print("ğŸ”¥ URL Scanner Blueprint ENABLED (end-to-end pipeline)")
    print("="*50)
    
    # News now fetches dynamically from NewsAPI - no startup fetch needed
    print("âœ… News system ready for dynamic NewsAPI fetching")
    
    print("\nğŸš€ Starting server...\n")
    socketio.run(app, host='0.0.0.0', port=5000, debug=False, use_reloader=False)

